---
permalink: /
title: ""
excerpt: "Gaurav Jain | CS PhD candidate, Columbia University"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---
<!-- ## About Me -->

Hi! I'm a third-year Ph.D. candidate in computer science at Columbia University. I work on Human-computer Interaction with Professor [Brian A. Smith](http://www.cs.columbia.edu/~brian/index.html) in the Computer-Enabled Abilities Laboratory ([CEAL](https://ceal.cs.columbia.edu/)).



## Research Summary

I build <strong>Human-AI systems</strong> that embed AI technologies (<i>e.g.,</i> computer vision) into new human interactions to solve accessibility problems. My research focuses on designing, developing, and evaluating systems that enable blind and low-vision (BLV) people to experience the world with more agency. 

Using my systems, BLV people can visualize the action in sports broadcasts for themselves rather than relying on other people's descriptions. The system automatically generates immersive audio representations of sports directly from source video broadcasts using computer vision ([ACM CHI EA 2023](https://dl.acm.org/doi/10.1145/3544549.3585610)). 

To inform future systems, I also conduct in-depth qualitative studies that introduce new assistance paradigms. For instance, my qualitative study on BLV people's navigation proposed the concept of "exploration assistance systems," that grant BLV people the freedom and spontaneity to explore unfamiliar environments by themselves rather than being guided by turn-by-turn instructions to their destination ([ACM CSCW 2023](https://dl.acm.org/doi/10.1145/3579496?cid=99659562550)).

<!-- introduced the concept of "exploration assistance," an evolution of current navigation systems --largely focused on turn-by-turn navigation-- to additionally provide blind and low-vision people the freedom to explore unfamiliar environments by making navigation decisions on the fly, rather than being guided via a fixed path ([ACM CSCW 2023](https://dl.acm.org/doi/10.1145/3579496?cid=99659562550))

To inform future systems that capture what is important to domain experts and people with disabilities, I also conduct and collaborate on in-depth qualitative -->

<!-- My research is focused on building human-AI systems to achieve inclusive physical and digital environments. To this end, I design, develop, and evaluate systems that embed AI --specifically, computer vision-- into human interactions with the goal of enabling people with disabilities to engage with the world with more agency.  -->
<!-- My work leverages AI not to automate interactions, instead it enables equivalent experiences to provide access to everyone, irrespective of their abilities.  -->

<!-- My work takes a different approach to leveraging AI for solving accessibility problems. Instead of using AI for automating or simplifying interactions for people with disabilities, I leverage AI for making it possible for people with disabilities to interact  -->
<!-- 
Previously, my work has introduced the concept of "exploration assistance," an evolution of current navigation systems --largely focused on turn-by-turn navigation-- to additionally provide blind and low-vision people the freedom to explore unfamiliar environments by making navigation decisions on the fly, rather than being guided via a fixed path ([ACM CSCW 2023](https://dl.acm.org/doi/10.1145/3579496?cid=99659562550)). In the realm of digital spaces, my work has introduced systems that automatically generate immersive audio representations of sports, making it possible for blind viewers to visualize the action in sports broadcasts for themselves rather than relying on descriptions from other people ([ACM CHI EA 2023](https://dl.acm.org/doi/10.1145/3544549.3585610)). My ongoing work explores the accessibility of videos  -->

<!-- I am interested in using AI -- specifically, computer vision and deep learning -- to address accessibility problems. My work is focused on designing and developing systems that help blind or low-vision (BLV) people better experience the world -- both physical and digital worlds -- around them.  

Previously, I have performed a qualitative study with BLV people to understand how navigation systems can evolve to support them in exploring unfamiliar environments, as opposed to just guiding them to their destination ([ACM CSCW 2023](https://arxiv.org/abs/2211.16465)). More recently, I developed a computer vision-based system that facilitates a more immersive experience for BLV people when watching sports, enabling them to visualize gameplay directly from videos (ACM CHI 2023).  -->

<!-- My ongoing work explores the use of multimodal representation learning and crowdsourcing to develop tools that help make videos accessible at scale. In addition, I'm exploring new forms of interactions that smart streetscapes — streets equipped with sensors and computational power — can facilitate to support BLV people's navigation.  -->





Here's my [CV](http://gaurav1302.github.io/files/Gaurav_CV_PhD.pdf).



## Research areas:

Human-Computer Interaction, Human-AI Systems, Accessibility, Computer Vision, Deep Learning.


## Talks

December 2022: "[Watching Videos Without Vision: Challenges, Techniques, and the Future of Video Accessibility](https://www.youtube.com/watch?v=QuGQKrjezdk)." 
              <!-- | [Slides](http://gaurav1302.github.io/files/CE.pdf) ) <a href= "https://www.youtube.com/watch?v=QuGQKrjezdk" target="_blank" style="text-decoration: none; font-size:11pt; margin-top: 5px; margin-bottom:0px; color: #333;">
                <span style="color: #52adc8; text-decoration: none; margin-right: 6px; padding: 5px 12px; background: rgba(0, 0, 0, 0.05); border: 1px solid #ccc; border-radius: 6px; line-height: 46px; white-space: nowrap; color: #303030; 
                ">
                  Talk
                </span>                
              </a>
              <a href= "http://gaurav1302.github.io/files/CE.pdf" target="_blank" style="text-decoration: none; font-size:11pt; margin-top: 5px; margin-bottom:0px; color: #333;">
                <span style="color: #52adc8; text-decoration: none; margin-right: 6px; padding: 5px 12px; background: rgba(0, 0, 0, 0.05); border: 1px solid #ccc; border-radius: 6px; line-height: 46px; white-space: nowrap; color: #303030; 
                ">
                  Slides
                </span>                
              </a> -->

<!-- <iframe width="160" height="90" src="https://www.youtube.com/embed/QuGQKrjezdk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
<!-- - November 2022: Paper accepted at CSCW 2023. [[Preprint](https://arxiv.org/abs/2211.16465)]  -->

<!-- My research revolves around building systems that help people with disabilities to better experience the world around them. Specifically, I am interested in leveraging computer vision and deep learning for multimodal analysis (such as videos, images, and audio) to make digital media more accessible to people who are blind and low vision. To this end, I focus on solving technical challenges of using AI for accessibility and the design of novel interaction techniques that help facilitate a more immersive user experience for people with disabilities. -->

<!-- ### Curriculum Vitae (CV) -->
<!-- Please find my CV  -->


## Selected Publications


{% include base_path %}
{% for post in site.publications reversed %}
  {% include archive-single-pub.html %}
{% endfor %}  

For an updated list of articles, please visit my [Google Scholar Profile](https://scholar.google.com/citations?user=piSn5gQAAAAJ&hl=en)


## Contact Me

If you're interested in my work and wish to discuss anything, feel free to email me (myfirstname \[at\] cs \[dot\] columbia \[dot\] edu) or connect with me on [LinkedIn](https://www.linkedin.com/in/gauravjain13298/) -- I'm open to new connections!

